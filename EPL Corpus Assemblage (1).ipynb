{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPL Corpus Creation\n",
    "\n",
    "This notebook is intending to create three CSV files from the [EPLEarly Text Corpus](https://earlyprint.org/lab/). The end result files from this notebook are three CSV files. The first, a split by paragraph corpus for all of the texts, the second a sentence split corpus, and the third a sentence split corpus by word with the modernized, adorned, and lemma for all of the texts.\n",
    "\n",
    "The texts can be found at EPL's [Bitbucket](https://bitbucket.org/eplib/eebotcp/src/master/), and downloaded if so desired. The purpose of this notebook is to have a main CSV file where the texts can instead be queried in a central location, and reducing further preprocessing and computational resources moving forward. I forked the BitBucket repository, to my personal account to reduce traffic and API calls from the EPL repository, I did not want to overload the servers hosting their data by accident!\n",
    "\n",
    "To do so, I will iteratively read in the files, download them, structure them in a dataframe, and then delete the files from the repository. This process will repeat until all files have been structured into the CSV files. The documentation for the BitBucket [Python API](https://atlassian-python-api.readthedocs.io/bitbucket.html) is linked here, additional code used to structure this data pull can be found at this [GitHub repository](https://github.com/atlassian-api/atlassian-python-api/tree/master/examples/bitbucket) provided by the BitBucket parent company, Atlassian.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê 1. Environment Creation\n",
    "\n",
    "### 1.1 Library Import\n",
    "\n",
    "The libraries used in this section fall under four categories: data access, data query, data structuring, and a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DATA ACCESS '''\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "''' DATA QUERY '''\n",
    "from lxml import etree\n",
    "parser = etree.XMLParser(collect_ids=False,encoding='utf-8')\n",
    "nsmap = {'tei': 'http://www.tei-c.org/ns/1.0'} ### EPL Source\n",
    "\n",
    "''' DATA STRUCTURING '''\n",
    "import pandas as pd\n",
    "\n",
    "''' PROGRESS BAR '''\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Storage Import\n",
    "\n",
    "The data will be iteratively pulled from two places, the machine file directory and the BitBucket Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' LOCAL FILE DIRECTORY '''\n",
    "\n",
    "files = glob.glob(r\"/scratch/alpine/naca4005/texts/*/*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(r\"/scratch/alpine/naca4005/texts/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/alpine/naca4005/texts/A07/A07811.xml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ2. XML Parsing\n",
    "\n",
    "To parse out the XML data, I will be using the documentation provided by the EPL team titled [Parsing *Early Print* XML Texts](https://earlyprint.org/jupyterbook/ep_xml.html). The author of this documentation was [John R Ladd](https://jrladd.com/). The EPL corpus is encoded usinig the [TEI simplePrint](https://tei-c.org/release/doc/tei-p5-exemplars/html/tei_simplePrint.doc.html) standard, and was referenced to extract the paragraph like divisons.\n",
    "\n",
    "At any reference to modernized words, this was conducted by the EPL using the [Northwestern Morphadorener](https://morphadorner.northwestern.edu/morphadorner/). This data was already preprocessed and prepared by the EPL team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' CREATING AN XML PARSER '''\n",
    "    ## Creating a parser object\n",
    "parser = etree.XMLParser(collect_ids=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "XMLSyntaxError",
     "evalue": "Document is empty, line 1, column 1 (A49, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/curc/sw/anaconda3/2020.11/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-7-c5f93185f8f4>\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    tree = etree.parse(files[1], parser)\n",
      "  File \u001b[1;32m\"src/lxml/etree.pyx\"\u001b[0m, line \u001b[1;32m3521\u001b[0m, in \u001b[1;35mlxml.etree.parse\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1859\u001b[0m, in \u001b[1;35mlxml.etree._parseDocument\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1885\u001b[0m, in \u001b[1;35mlxml.etree._parseDocumentFromURL\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1789\u001b[0m, in \u001b[1;35mlxml.etree._parseDocFromFile\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1177\u001b[0m, in \u001b[1;35mlxml.etree._BaseParser._parseDocFromFile\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m615\u001b[0m, in \u001b[1;35mlxml.etree._ParserContext._handleParseResultDoc\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m725\u001b[0m, in \u001b[1;35mlxml.etree._handleParseResult\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"src/lxml/parser.pxi\"\u001b[0;36m, line \u001b[0;32m654\u001b[0;36m, in \u001b[0;35mlxml.etree._raiseParseError\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/scratch/alpine/naca4005/texts/A49\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31mXMLSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Document is empty, line 1, column 1\n"
     ]
    }
   ],
   "source": [
    "   ## Parse your XML file into a \"tree\" object - \n",
    "    ## the tree objects will be used iteratively later to generate data for each of the text files\n",
    "tree = etree.parse(files[1], parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' root generator function:\n",
    "\n",
    "    INPUT: File Path from your machine\n",
    "    OUTPUT: A parseable xml element that is TEI encoded\n",
    "\n",
    "    The purpose of this function is to simplify the steps needed to get an xml file in parseable state. Any file name that\n",
    "    is XML encoded will work for this function. \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def root_generator(file_path):\n",
    "    tree = etree.parse(file_path,parser)\n",
    "    \n",
    "    text_root = tree.getroot()\n",
    "    \n",
    "    return(text_root)\n",
    "\n",
    "''' root generator function:\n",
    "\n",
    "    INPUT: A text root\n",
    "    OUTPUT: The TEI tag to identify this book\n",
    "\n",
    "    The purpose of this function is to extract the unique TEI tag from the file. It will return a string with the TEI inside \n",
    "\n",
    "'''\n",
    "\n",
    "def tei_finder(text_root):\n",
    "    tei_tag = text_root.findall(\".//tei:idno[@ type='DLPS']\",namespaces=nsmap)\n",
    "    tei = [tag.text for tag in tei_tag]\n",
    "    return (tei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploratory Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a00001 = root_generator(files[0])\n",
    "a00002 = root_generator(files[1])\n",
    "a00003 = root_generator(files[2])\n",
    "a00004 = root_generator(files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A07480\n"
     ]
    }
   ],
   "source": [
    "root = root_generator(files[20])\n",
    "tei = tei_finder(root)\n",
    "\n",
    "print (tei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Extracting All Words\n",
    "\n",
    "Functions:\n",
    "\n",
    "* extract_all_words\n",
    "* extract_all_modernized_words\n",
    "* extract_all_lemmas\n",
    "* extract_all_pos\n",
    "\n",
    "All of the functions take the text root defined by the root generator above, and return a list of words with the desired type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' extract_all_words function:\n",
    "\n",
    "    This function is intended to extract all of the words from the text file. \n",
    "    This will include the title, author, publisher, and all other metadata that may be included in the text. \n",
    "    \n",
    "    INPUT: an xml tree tag\n",
    "    OUTPUT: a list containing all of the words\n",
    "\n",
    "'''\n",
    "def extract_all_words(text_root):\n",
    "    all_word_tags = text_root.findall(\".//tei:w\",namespaces=nsmap)\n",
    "    \n",
    "    all_words = [w.text for w in all_word_tags]\n",
    "    \n",
    "    return(all_words)\n",
    "\n",
    "\n",
    "''' extract_all_modernized_words function:\n",
    "\n",
    "    This function will generate a list of the modernized words generated by the NUIT Morphadorner. This function performs similarly\n",
    "    to the orignal text function above, but just pulls a different XML tag.\n",
    "    \n",
    "    INPUT: an xml tree tag\n",
    "    OUTPUT: a list containing all of the modernized words\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_all_modernized_words(text_root):\n",
    "    all_word_tags = text_root.findall(\".//tei:w\",namespaces=nsmap)\n",
    "    \n",
    "    regularized_words = [w.get('reg', w.text) for w in all_word_tags]\n",
    "    \n",
    "    \n",
    "    return(regularized_words)\n",
    "\n",
    "''' extract_all_lemmas function:\n",
    "\n",
    "    This function will generate a list of the lemmatized words by EPL. It works in a similar fashion to those above.\n",
    "    \n",
    "    INPUT: an xml tree tag\n",
    "    OUTPUT: a list containing all of the lemmatized words\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_all_lemmas(text_root):\n",
    "    all_word_tags = text_root.findall(\".//tei:w\",namespaces=nsmap)\n",
    "    \n",
    "    lemmas = [w.get('lemma') for w in all_word_tags]\n",
    "    \n",
    "    \n",
    "    return(lemmas)\n",
    "\n",
    "\n",
    "''' extract_all_pos function:\n",
    "\n",
    "    This function will generate a list of the Parts of Speech Tags attributed to each individual word. These were generated\n",
    "    \n",
    "    INPUT: an xml tree tag\n",
    "    OUTPUT: a list containing all of the POS tags for each respective word\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_all_pos(text_root):\n",
    "    all_word_tags = text_root.findall(\".//tei:w\",namespaces=nsmap)\n",
    "    \n",
    "    pos = [w.get('pos') for w in all_word_tags]\n",
    "    \n",
    "    return(pos)\n",
    "\n",
    "\n",
    "                                 \n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DAVID',\n",
       " 'sends',\n",
       " 'his',\n",
       " 'PIETIE',\n",
       " 'TO',\n",
       " 'KING',\n",
       " 'CHARLES',\n",
       " 'His',\n",
       " 'Subjects',\n",
       " 'Being']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = extract_all_words(a00004)\n",
    "all_words[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DAVID',\n",
       " 'sends',\n",
       " 'his',\n",
       " 'piety',\n",
       " 'TO',\n",
       " 'KING',\n",
       " 'CHARLES',\n",
       " 'His',\n",
       " 'Subjects',\n",
       " 'Being']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modernized_words = extract_all_modernized_words(a00004)\n",
    "modernized_words[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david',\n",
       " 'send',\n",
       " 'his',\n",
       " 'piety',\n",
       " 'to',\n",
       " 'king',\n",
       " 'charles',\n",
       " 'his',\n",
       " 'subject',\n",
       " 'be']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_words = extract_all_lemmas(a00004)\n",
    "lemma_words[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n1', 'acp', 'd', 'j', 'j', 'nn1', 'nn1', 'n1', 'acp', 'n2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_of_speech = extract_all_pos(a00004)\n",
    "parts_of_speech[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3950\n",
      "3950\n",
      "3950\n",
      "3950\n"
     ]
    }
   ],
   "source": [
    "print (len(all_words))\n",
    "print (len(modernized_words))\n",
    "print (len(lemma_words))\n",
    "print (len(parts_of_speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Extracting Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' extract_all_sentences function:\n",
    "\n",
    "    This function will generate a list of the lines coded a 'l' in the TEI XML files\n",
    "    \n",
    "    INPUT: an xml tree tag\n",
    "    OUTPUT: a list containing strings of all of the 'l' seperated lines.\n",
    "    \n",
    "'''\n",
    "def extract_all_lines(text_root):\n",
    "    line_tags = text_root.findall(\".//tei:l\", namespaces=nsmap)\n",
    "\n",
    "    all_lines = []\n",
    "    for line in line_tags: #Only the first 20 lines\n",
    "        current_line = (' '.join([w.text for w in line.findall(\".//tei:w\", namespaces=nsmap)]))\n",
    "        all_lines.append(current_line)\n",
    "        \n",
    "    return(all_lines)\n",
    "\n",
    "''' extract_sentences_punctuation function:\n",
    "\n",
    "    This function will generate a list of the lines coded a 'l' in the TEI XML files.\n",
    "    Different than extract_all_sentences, the extract_sentences_punctuation will keep the punctuation provided\n",
    "    \n",
    "    INPUT: an xml tree tag\n",
    "    OUTPUT: a list containing strings of all of the 'l' seperated lines.\n",
    "    \n",
    "'''\n",
    "def extract_line_punctuation(text_root):\n",
    "    line_tags = text_root.findall(\".//tei:l\", namespaces=nsmap)\n",
    "\n",
    "    all_lines = []\n",
    "    for line in line_tags:\n",
    "        current_line = (' '.join([child.text for child in line]))\n",
    "        all_lines.append(current_line)\n",
    "        \n",
    "    return(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tags = a00001.findall(\".//tei:l\", namespaces=nsmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = extract_all_lines(a00004)\n",
    "lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuated_lines = extract_line_punctuation(a00004)\n",
    "punctuated_lines[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Extracting Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' extract_all_sentences function:\n",
    "\n",
    "    INPUT: an xml text root\n",
    "    OUTPUT: a list of strings that are each a sentence in the text\n",
    "    \n",
    "    This function is heavily adopted from the EPL documentation linked above. Naming conventions were changed from 'master'\n",
    "    to 'main', and the code comments were changed as well to increased readability in this file. This function, like the word\n",
    "    functions, will take an xml text root and geneare a split list based on the text provided. This function can take some\n",
    "    time to run, as it iterates through the tags one by one.\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_all_sentences(text_root):\n",
    "    word_and_punctuation_tags = text_root.xpath(\"//tei:w|//tei:pc\", namespaces=nsmap)\n",
    "    \n",
    "    \n",
    "        ## Creating list storage containers for the working text\n",
    "    all_sentences = []\n",
    "    new_sentence = []\n",
    "    \n",
    "        ## Iterating through each tag\n",
    "    \n",
    "    for tag in word_and_punctuation_tags:\n",
    "        ## First, we will test to see if the tag has the attribute to find the end of the sentence\n",
    "        if 'unit' in tag.attrib and tag.get('unit') == 'sentence':\n",
    "            if tag.text != None:\n",
    "                ## Adding the punctuation to the sentence\n",
    "                new_sentence.append(tag.text)\n",
    "            \n",
    "            joined_sentence = ' '.join([word for word in new_sentence if word is not None])\n",
    "            ## Storing the sentences\n",
    "            all_sentences.append(joined_sentence)\n",
    "            ## Reinstantiating the working sentence storage container\n",
    "            new_sentence = []\n",
    "        # If the tag is not at the end of a sentence, we can simply add its contents to the list\n",
    "        else:\n",
    "            new_sentence.append(tag.text)\n",
    "            \n",
    "    \n",
    "    return (all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercurius Davidicus , OR A Patterne of Loyall Devotion .\n",
      "Wherein King DAVID sends his PIETIE TO KING CHARLES , His Subjects .\n",
      "Being the practice of the Primitive Christians , Martyrs , and Confessors , in all Ages ; Very fitting to be used both publick and private in these disloyall TIMES .\n",
      "Likewise Prayers and Thanksgivings used in the Kings Army before and after BATTELL .\n",
      "Published by His Majesties Command .\n",
      "Oxford , Printed by Leonard Leichfield .\n",
      "1634.\n"
     ]
    }
   ],
   "source": [
    "sentences = extract_all_sentences(a00004)\n",
    "for sentence in sentences[0:7]:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Structured Parsing\n",
    "\n",
    "In this section, I will create three workflows to generate CSV files with the text. The first will be the word split, the second the sentence split, and the third the paragraph split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Word Split Parsing\n",
    "\n",
    "\n",
    "* extract_all_words\n",
    "* extract_all_modernized_words\n",
    "* extract_all_lemmas\n",
    "* extract_all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6e62bc72004a7297f7a48b522140e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='üìñüîç \\U0001fa84 speed reading...'), FloatProgress(value=0.0, max=489.0), HTML(value='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú®üìö data has been found\n"
     ]
    }
   ],
   "source": [
    "text_data = [] # Empty list for data\n",
    "teis = [] # Empty list for TCP IDs\n",
    "\n",
    "## extracting the metadata from each file\n",
    "for file_name in tqdm(files,desc=\"üìñüîç ü™Ñ speed reading...\",unit=' text',):\n",
    "        ## Finding TEI and creating a parse object\n",
    "    root = root_generator(file_name)\n",
    "    tei = tei_finder(root)\n",
    "    teis.append(tei)\n",
    "    \n",
    "        ## Parsing the object using the above functions\n",
    "        \n",
    "    words = extract_all_words(root)\n",
    "    modernized = extract_all_modernized_words(root)\n",
    "    lemmas = extract_all_lemmas(root)\n",
    "    pos = extract_all_pos(root)\n",
    "\n",
    "    current_text = {'TEI':tei,'words':words,'modernized':modernized,'lemmas':lemmas,'pos':pos}\n",
    "    text_data.append(current_text)\n",
    "      ## Adding the tcp to the index list\n",
    "    \n",
    "    \n",
    "    \n",
    "print (\"‚ú®üìö data has been found\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEI</th>\n",
       "      <th>words</th>\n",
       "      <th>modernized</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>[¬∂, A, deuout, treatise, vpon, the, Pater, nos...</td>\n",
       "      <td>[¬∂, A, devout, treatise, upon, the, Pater, nos...</td>\n",
       "      <td>[¬∂, a, devout, treatise, upon, the, n/a, n/a, ...</td>\n",
       "      <td>[sy, d, j, n1, acp, d, fla, fla, vvn, ord, acp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00544</td>\n",
       "      <td>[A, DISCOVERY, OF, THE, ABHOminable, Delusions...</td>\n",
       "      <td>[A, DISCOVERY, OF, THE, Abominable, Delusions,...</td>\n",
       "      <td>[a, discovery, of, the, abominable, delusion, ...</td>\n",
       "      <td>[d, n1, acp, d, j, n2, acp, d, crq, vvb, pr, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00630</td>\n",
       "      <td>[THE, ARTES, OF, LOGIKE, AND, Rethorike, plain...</td>\n",
       "      <td>[THE, ARTS, OF, LOGIC, AND, Rhetoric, plainly,...</td>\n",
       "      <td>[the, art, of, logic, and, rhetoric, plain, se...</td>\n",
       "      <td>[d, n2, acp, n1, cc, n1, av-j, vvn, av, acp, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00174</td>\n",
       "      <td>[ARTICLES, TO, BE, MINISTRED, ENQVIRED, OF, AN...</td>\n",
       "      <td>[ARTICLES, TO, BE, ministered, ENQVIRED, OF, A...</td>\n",
       "      <td>[article, to, be, minister, enqvire, of, and, ...</td>\n",
       "      <td>[n2, acp, vvb, vvn, vvn, acp, cc, vvd, acp, d,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00341</td>\n",
       "      <td>[THE, COMPARATION, OF, a, Uyrgin, and, a, Mart...</td>\n",
       "      <td>[THE, COMPARATION, OF, a, Uyrgin, and, a, Mart...</td>\n",
       "      <td>[the, comparation, of, a, uyrgin, and, a, mart...</td>\n",
       "      <td>[d, n1, acp, d, n1, cc, d, n1, ab, crd, sy, d,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEI                                              words  \\\n",
       "0  A00361  [¬∂, A, deuout, treatise, vpon, the, Pater, nos...   \n",
       "1  A00544  [A, DISCOVERY, OF, THE, ABHOminable, Delusions...   \n",
       "2  A00630  [THE, ARTES, OF, LOGIKE, AND, Rethorike, plain...   \n",
       "3  A00174  [ARTICLES, TO, BE, MINISTRED, ENQVIRED, OF, AN...   \n",
       "4  A00341  [THE, COMPARATION, OF, a, Uyrgin, and, a, Mart...   \n",
       "\n",
       "                                          modernized  \\\n",
       "0  [¬∂, A, devout, treatise, upon, the, Pater, nos...   \n",
       "1  [A, DISCOVERY, OF, THE, Abominable, Delusions,...   \n",
       "2  [THE, ARTS, OF, LOGIC, AND, Rhetoric, plainly,...   \n",
       "3  [ARTICLES, TO, BE, ministered, ENQVIRED, OF, A...   \n",
       "4  [THE, COMPARATION, OF, a, Uyrgin, and, a, Mart...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [¬∂, a, devout, treatise, upon, the, n/a, n/a, ...   \n",
       "1  [a, discovery, of, the, abominable, delusion, ...   \n",
       "2  [the, art, of, logic, and, rhetoric, plain, se...   \n",
       "3  [article, to, be, minister, enqvire, of, and, ...   \n",
       "4  [the, comparation, of, a, uyrgin, and, a, mart...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [sy, d, j, n1, acp, d, fla, fla, vvn, ord, acp...  \n",
       "1  [d, n1, acp, d, j, n2, acp, d, crq, vvb, pr, d...  \n",
       "2  [d, n2, acp, n1, cc, n1, av-j, vvn, av, acp, d...  \n",
       "3  [n2, acp, vvb, vvn, vvn, acp, cc, vvd, acp, d,...  \n",
       "4  [d, n1, acp, d, n1, cc, d, n1, ab, crd, sy, d,...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tei_data = pd.DataFrame(text_data)\n",
    "tei_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEI</th>\n",
       "      <th>words</th>\n",
       "      <th>modernized</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>¬∂</td>\n",
       "      <td>¬∂</td>\n",
       "      <td>¬∂</td>\n",
       "      <td>sy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>deuout</td>\n",
       "      <td>devout</td>\n",
       "      <td>devout</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>treatise</td>\n",
       "      <td>treatise</td>\n",
       "      <td>treatise</td>\n",
       "      <td>n1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>vpon</td>\n",
       "      <td>upon</td>\n",
       "      <td>upon</td>\n",
       "      <td>acp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEI     words modernized    lemmas  pos\n",
       "0  A00361         ¬∂          ¬∂         ¬∂   sy\n",
       "0  A00361         A          A         a    d\n",
       "0  A00361    deuout     devout    devout    j\n",
       "0  A00361  treatise   treatise  treatise   n1\n",
       "0  A00361      vpon       upon      upon  acp"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data = tei_data.apply(pd.Series.explode)\n",
    "word_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Sentence Split Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263d884506044154a8a9aee27ed3c48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='üìñüîç \\U0001fa84 speed reading...'), FloatProgress(value=0.0, max=10.0), HTML(value=''‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú®üìö data has been found\n"
     ]
    }
   ],
   "source": [
    "sentence_data = [] # Empty list for data\n",
    "teis = [] # Empty list for TCP IDs\n",
    "\n",
    "## extracting the metadata from each file\n",
    "for file_name in tqdm(files[0:10],desc=\"üìñüîç ü™Ñ speed reading...\",unit=' text',):\n",
    "        ## Finding TEI and creating a parse object\n",
    "    root = root_generator(file_name)\n",
    "    tei = tei_finder(root)\n",
    "    teis.append(tei)\n",
    "    \n",
    "        ## Parsing the object using the above functions\n",
    "    sentences = extract_all_sentences(root)\n",
    "\n",
    "    current_text = {'TEI':tei,'sentences':sentences}\n",
    "    sentence_data.append(current_text)\n",
    "    \n",
    "    \n",
    "print (\"‚ú®üìö data has been found\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEI</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>[¬∂ A deuout treatise vpon the Pater noster / m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00544</td>\n",
       "      <td>[A DISCOVERY OF THE ABHOminable Delusions of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00630</td>\n",
       "      <td>[THE ARTES OF LOGIKE AND Rethorike , plainlie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00174</td>\n",
       "      <td>[ARTICLES TO BE MINISTRED , ENQVIRED OF , AND ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00341</td>\n",
       "      <td>[THE COMPARATION OF a Uyrgin and a Martyr ., A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEI                                          sentences\n",
       "0  A00361  [¬∂ A deuout treatise vpon the Pater noster / m...\n",
       "1  A00544  [A DISCOVERY OF THE ABHOminable Delusions of t...\n",
       "2  A00630  [THE ARTES OF LOGIKE AND Rethorike , plainlie ...\n",
       "3  A00174  [ARTICLES TO BE MINISTRED , ENQVIRED OF , AND ...\n",
       "4  A00341  [THE COMPARATION OF a Uyrgin and a Martyr ., A..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tei_full_sentences = pd.DataFrame(sentence_data)\n",
    "tei_full_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEI</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>¬∂ A deuout treatise vpon the Pater noster / ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>¬∂ Richarde Hyrde / vnto the moost studyous and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>S. sendeth gretynge and well to fare .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>I Haue herde many men put great dout‚óè whether ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00361</td>\n",
       "      <td>And some vtterly affyrme that it is nat onely ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEI                                          sentences\n",
       "0  A00361  ¬∂ A deuout treatise vpon the Pater noster / ma...\n",
       "0  A00361  ¬∂ Richarde Hyrde / vnto the moost studyous and...\n",
       "0  A00361             S. sendeth gretynge and well to fare .\n",
       "0  A00361  I Haue herde many men put great dout‚óè whether ...\n",
       "0  A00361  And some vtterly affyrme that it is nat onely ..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tei_sentences = tei_full_sentences.explode(column='sentences')\n",
    "tei_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 3. Saving The Outputs\n",
    "\n",
    "The files generated above will be saved in a similar way to the repository naming conventions found in BitBucket. The file name will have the first three of the TEI id (ie. A00, B00 ...) and then will have a descriptive name.\n",
    "\n",
    "* *Text Data*: Each row is a different text, the words are all stored in lists in the rows\n",
    "* *By Word Data*: Each row is a different word from all of the texts, rows are ordered chronologically and TEI least to greatest.\n",
    "* *Sentence Data*: Each row is a different text, the sentences are all stored in lists in the rows\n",
    "* *By Sentence Data*: Each row is an individual sentence from all of the texts. Rows are ordered chronologically, and TEI least to greatest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## Saving these to a similar naming convention that the oringinal EPL uses\n",
    "repo_tei_code_first = tei_data.at[0,'TEI']\n",
    "repo_tei_code = repo_tei_code_first[0:3]\n",
    "\n",
    "text_file_name = str(repo_tei_code)+\" Text Data.csv\"\n",
    "by_word_file_name = str(repo_tei_code)+ \" By Word Data.csv\"\n",
    "sentence_full_file_name = str(repo_tei_code)+\" Sentence Data.csv\"\n",
    "sentence_file_name = str(repo_tei_code)+\" By Sentence Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## Saving the files to the current directory\n",
    "\n",
    "tei_data.to_csv(text_file_name)\n",
    "word_data.to_csv(by_word_file_name)\n",
    "tei_full_sentences.to_csv(sentence_full_file_name)\n",
    "tei_sentences.to_csv(sentence_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ûø 4. Function Creation\n",
    "\n",
    "This section will combine everything above into a function so that a block of code per CSV needed will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_word_dataframe_maker(files):\n",
    "    print (\"üìéWORD PARSING\")\n",
    "    text_data = [] # Empty list for data\n",
    "    \n",
    "    ## extracting the metadata from each file\n",
    "    for file_name in files:\n",
    "        #print (file_name)\n",
    "            ## Finding TEI and creating a parse object\n",
    "        root = root_generator(file_name)\n",
    "        tei = tei_finder(root)\n",
    "\n",
    "            ## Parsing the object using the above functions\n",
    "\n",
    "        words = extract_all_words(root)\n",
    "        modernized = extract_all_modernized_words(root)\n",
    "        lemmas = extract_all_lemmas(root)\n",
    "        pos = extract_all_pos(root)\n",
    "\n",
    "        current_text = {'TEI':tei,'words':words,'modernized':modernized,'lemmas':lemmas,'pos':pos}\n",
    "        text_data.append(current_text)\n",
    "          \n",
    "\n",
    "    print (\"‚ú®üìö data has been found\\n... dataframes being assembled\")\n",
    "    tei_data = pd.DataFrame(text_data)\n",
    "    word_data = tei_data.apply(pd.Series.explode)\n",
    "    \n",
    "   \n",
    "    \n",
    "    print(\"‚úíÔ∏è... writing files to directory\")\n",
    "    repo_tei_code_first = tei_data.at[0,'TEI']\n",
    "    repo_tei_code = repo_tei_code_first[0:3]\n",
    "\n",
    "    text_file_name = str(repo_tei_code)+\" FULL Text Data.csv\"\n",
    "    by_word_file_name = str(repo_tei_code)+ \" SPLIT Word Data.csv\"\n",
    "    \n",
    "    tei_data.to_csv(text_file_name)\n",
    "    word_data.to_csv(by_word_file_name)\n",
    "    \n",
    "    print (f\"ü§ñüí¨ all files in the {repo_tei_code} have been proccessed and saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_sentence_dataframe_maker(files):\n",
    "    print (\"üìéSENTENCE PARSING\")\n",
    "    sentence_data = [] # Empty list for data\n",
    "    teis = [] # Empty list for TCP IDs\n",
    "\n",
    "    ## extracting the metadata from each file\n",
    "    for file_name in files:\n",
    "            ## Finding TEI and creating a parse object\n",
    "        root = root_generator(file_name)\n",
    "        tei = tei_finder(root)\n",
    "        teis.append(tei)\n",
    "\n",
    "            ## Parsing the object using the above functions\n",
    "        sentences = extract_all_sentences(root)\n",
    "\n",
    "        current_text = {'TEI':tei,'sentences':sentences}\n",
    "        sentence_data.append(current_text)\n",
    "\n",
    "\n",
    "    print (\"‚ú®üìö data has been found\\n...dataframes are being assembled\")\n",
    "    tei_full_sentences = pd.DataFrame(sentence_data)\n",
    "    tei_sentences = tei_full_sentences.explode(column='sentences')\n",
    "\n",
    "    \n",
    "    print(\"‚úíÔ∏è... writing files to directory\")\n",
    "    repo_tei_code_first = tei_full_sentences.at[0,'TEI']\n",
    "    repo_tei_code = repo_tei_code_first[0:3]\n",
    "    \n",
    "    sentence_full_file_name = str(repo_tei_code)+\" FULL Sentence Data.csv\"\n",
    "    sentence_file_name = str(repo_tei_code)+\" SPLIT Sentence Data.csv\"\n",
    "    tei_full_sentences.to_csv(sentence_full_file_name)\n",
    "    tei_sentences.to_csv(sentence_file_name)\n",
    "    \n",
    "    print (f\"ü§ñüí¨ all files in the {repo_tei_code} have been proccessed and saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_iterator(path):\n",
    "    folder_path = glob.glob(r\"/scratch/alpine/naca4005/texts/\"+path)\n",
    "    for folder in folder_path:\n",
    "        xml_paths = folder+\"/*.xml\"\n",
    "        xml_files = glob.glob(xml_paths)\n",
    "        \n",
    "        #by_word_dataframe_maker(xml_files)\n",
    "        by_sentence_dataframe_maker(xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìéWORD PARSING\n",
      "‚ú®üìö data has been found\n",
      "... dataframes being assembled\n",
      "‚úíÔ∏è... writing files to directory\n",
      "ü§ñüí¨ all files in the A02 have been proccessed and saved successfully\n",
      "üìéSENTENCE PARSING\n"
     ]
    }
   ],
   "source": [
    "folder_iterator(\"A02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìéSENTENCE PARSING\n",
      "‚ú®üìö data has been found\n",
      "...dataframes are being assembled\n",
      "‚úíÔ∏è... writing files to directory\n",
      "ü§ñüí¨ all files in the A02 have been proccessed and saved successfully\n"
     ]
    }
   ],
   "source": [
    "folder_iterator(\"A02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üÜïA07\n",
      "üìéWORD PARSING\n",
      "‚ú®üìö data has been found\n",
      "... dataframes being assembled\n",
      "‚úíÔ∏è... writing files to directory\n",
      "ü§ñüí¨ all files in the A07 have been proccessed and saved successfully\n",
      "üìéSENTENCE PARSING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6ae6df8f740a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüÜï{path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfolder_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-9d5e020eb4fc>\u001b[0m in \u001b[0;36mfolder_iterator\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mby_word_dataframe_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mby_sentence_dataframe_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-319b448a6a8c>\u001b[0m in \u001b[0;36mby_sentence_dataframe_maker\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m## Parsing the object using the above functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_all_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcurrent_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'TEI'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtei\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fd445afd050d>\u001b[0m in \u001b[0;36mextract_all_sentences\u001b[0;34m(text_root)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_all_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mword_and_punctuation_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//tei:w|//tei:pc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in dir_list[0:4]:\n",
    "    print (f\"\\nüÜï{path}\")\n",
    "    folder_iterator(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. API Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TEI</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A02262</td>\n",
       "      <td>CHRISTS PASSION .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A02262</td>\n",
       "      <td>A TRAGEDIE .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A02262</td>\n",
       "      <td>WITH ANNOTATIONS .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A02262</td>\n",
       "      <td>LONDON , Printed by Iohn Legatt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A02262</td>\n",
       "      <td>M. D. C. XL. M.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722170</th>\n",
       "      <td>524</td>\n",
       "      <td>A02403</td>\n",
       "      <td>That which is objected chiefly by covetous and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722171</th>\n",
       "      <td>524</td>\n",
       "      <td>A02403</td>\n",
       "      <td>Had indeed his Majestie any way ayded eyther t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722172</th>\n",
       "      <td>524</td>\n",
       "      <td>A02403</td>\n",
       "      <td>But now seeing his Majestie has beene alwayes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722173</th>\n",
       "      <td>524</td>\n",
       "      <td>A02403</td>\n",
       "      <td>Wherefore seeing his Majestie protests he ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722174</th>\n",
       "      <td>524</td>\n",
       "      <td>A02403</td>\n",
       "      <td>FINIS .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722175 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     TEI                                          sentences\n",
       "0                0  A02262                                  CHRISTS PASSION .\n",
       "1                0  A02262                                       A TRAGEDIE .\n",
       "2                0  A02262                                 WITH ANNOTATIONS .\n",
       "3                0  A02262                  LONDON , Printed by Iohn Legatt .\n",
       "4                0  A02262                                    M. D. C. XL. M.\n",
       "...            ...     ...                                                ...\n",
       "722170         524  A02403  That which is objected chiefly by covetous and...\n",
       "722171         524  A02403  Had indeed his Majestie any way ayded eyther t...\n",
       "722172         524  A02403  But now seeing his Majestie has beene alwayes ...\n",
       "722173         524  A02403  Wherefore seeing his Majestie protests he ente...\n",
       "722174         524  A02403                                            FINIS .\n",
       "\n",
       "[722175 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"A02 SPLIT Sentence Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
