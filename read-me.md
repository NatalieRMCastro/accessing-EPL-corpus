This page explains how the EPL corpus was collected, processed, and trained for use by the CU TECH Lab. The outline of this notebook consists of three phases. First, the metadata was collected from the provided metadata found on the EPL website. Next, mass downloading the text into our local research computing and transferring it into the correct file structure. This text was the processed using a research computing batch script to obtain the data in both split by token and split by sentence forms. Finally, the corpora were then combined with the metadata to result in one large (52,452,244 sentences!) CSV for the sentneces.

The goal of this processing is two fold. Our first goal is to train a BERTopic Model on the text, and second is to conduct further analysis such as RTF triple idenfication for narrative detection. The files used to process the texts can be found here, but due to the scope of the processing, using research computing is reccomended to access and process the entire EPL corpus. The corpus has over 60,000 texts dating before the 1800s with part of speech tagging, morphadorned text, and lemmatized options.
